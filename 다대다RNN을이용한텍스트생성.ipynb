{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "다대다RNN을이용한텍스트생성.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHNTjMOBlZk/ZvrXXdxaMi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeenhyung/mnist_example/blob/main/%EB%8B%A4%EB%8C%80%EB%8B%A4RNN%EC%9D%84%EC%9D%B4%EC%9A%A9%ED%95%9C%ED%85%8D%EC%8A%A4%ED%8A%B8%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W19bDBGvxLL_"
      },
      "source": [
        "# 11. 다대다 RNN을 이용한 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTPokqTlx3j8"
      },
      "source": [
        "## 01. 문자 단위 RNN(Char RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_y9Bg4Tx5Ei"
      },
      "source": [
        "### 1. 문자 단위 RNN(Char RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBGbkIexx6wa"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klhiVXc-yoX-"
      },
      "source": [
        "1. 훈련 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQykfUlQG2Nb",
        "outputId": "41339e01-dd0e-4075-892f-ce10a5572e20"
      },
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1c7NVi1HZO8"
      },
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJqtbMv-IBZ6",
        "outputId": "e1b23863-bfed-4b6d-99dc-f16c4dfe0e6a"
      },
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kghaIf2IGWl",
        "outputId": "48498a14-0232-4510-bf84-3dadf7a5f94d"
      },
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB_pNolLJVRX",
        "outputId": "d297ded4-3794-450b-b4f0-2de0c03a52af"
      },
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjSKUtMFJcyA",
        "outputId": "e2bdde03-42c1-4dd3-c1e7-a17f04e6c2ea"
      },
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn9G6HSvJpIS",
        "outputId": "9a0658e9-a66f-4ca1-df9a-51d0ebacecd9"
      },
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtDKnRw4KEH8"
      },
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VfGxkXAKMDi",
        "outputId": "0e09431f-6f71-475b-b644-0f9af5963ae6"
      },
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIhOW823Ka6Y"
      },
      "source": [
        "2. 모델 구현하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEpjRmcsKc85"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvAFXmMKpaG"
      },
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAhMcvx5Kq2A",
        "outputId": "5a60326d-59cd-42b3-f48e-e26b4d5599c9"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서\n",
        "print(outputs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 5])\n",
            "tensor([[[-0.3864, -0.5620,  0.0627, -0.3194,  0.1147],\n",
            "         [-0.0140, -0.5549,  0.3876, -0.2794, -0.0017],\n",
            "         [-0.0784, -0.5803,  0.3018, -0.4570,  0.1659],\n",
            "         [-0.2652, -0.5755,  0.3205, -0.5189,  0.2864],\n",
            "         [-0.5456, -0.5825,  0.0858, -0.4709,  0.1652]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic7qQvkUKsez",
        "outputId": "e9c219f5-4801-4e4f-bc81-ff172af0cee9"
      },
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd1uGjPWKzPv",
        "outputId": "24aaf22a-f3c9-4d28-cbdb-f28db75bb3e8"
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y0NKpKBK7-3"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXVr3CGjK94B",
        "outputId": "1018ab46-055a-48bf-e0a5-b5d2f1834248"
      },
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss:  1.6051690578460693 prediction:  [[4 2 2 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  peeep\n",
            "1 loss:  1.3574752807617188 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.1851308345794678 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "3 loss:  1.016121506690979 prediction:  [[4 4 4 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppep\n",
            "4 loss:  0.8429397344589233 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.6695433259010315 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "6 loss:  0.5153540372848511 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.390146404504776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.2858949303627014 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.20134322345256805 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.14013835787773132 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.09877242147922516 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.07027396559715271 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.05022432655096054 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.036134399473667145 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.026268159970641136 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.019326288253068924 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.014437070116400719 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.01109863631427288 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.008969414979219437 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.007600544486194849 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.006461009383201599 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.005336669273674488 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.004345993045717478 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.003577986266463995 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0030127218924462795 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0025970640126615763 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.00228447699919343 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.002042209729552269 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0018487550551071763 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0016899693291634321 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0015566395595669746 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0014427563874050975 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0013440430629998446 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0012576004955917597 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0011813135351985693 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.0011137090623378754 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0010535030160099268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0009996972512453794 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0009516977588646114 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0009086483041755855 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0008701924234628677 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0008356639882549644 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.0008046111324802041 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0007767006754875183 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0007515759207308292 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0007289035129360855 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0007084216922521591 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0006898686988279223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0006729586748406291 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0006575727602466941 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0006434728275053203 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0006306112627498806 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0006187498802319169 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.000607841182500124 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0005976706743240356 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.00058833381626755 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0005795208271592855 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0005713509162887931 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0005637049907818437 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0005565591854974627 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0005497944657690823 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0005434583872556686 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.0005373843014240265 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0005317151662893593 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0005262602353468537 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0005210674135014415 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0005161365261301398 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.0005114200757816434 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0005068702157586813 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0005024871206842363 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0004983184044249356 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0004942925297655165 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0004903619410470128 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.00048659805906936526 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.00048292946303263307 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0004794752167072147 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0004760209412779659 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.00047266195178963244 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0004694220260716975 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0004662773571908474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.00046320416731759906 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.000460202427348122 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.00045724824303761125 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.00045434170169755816 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.00045155431143939495 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0004488145059440285 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.00044612231431528926 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0004434778238646686 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.000440952368080616 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.00043837929842993617 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0004358300066087395 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0004333998076617718 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0004309934447519481 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.00042858702363446355 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0004262997827026993 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0004239886184222996 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.000421701290179044 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.00041943779797293246 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.00041726959170773625 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVi1O0QZePEX"
      },
      "source": [
        "# 02. 문자 단위 RNN(Char RNN) - 더 많은 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztXdJQpgeSUU"
      },
      "source": [
        "## 2. 문자 단위 RNN(Char RNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y6YAEQ6eUPy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ-u0Ph8eXSH"
      },
      "source": [
        "### 1. 훈련 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcW8K4hzeZW9"
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iDcGnkjg_N2"
      },
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NLpcnIzhE69",
        "outputId": "a8edf3cd-89b8-4631-ea35-3383da6c5a79"
      },
      "source": [
        "print(char_dic) # 공백도 여기서는 하나의 원소"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'c': 0, 'p': 1, 'm': 2, 'n': 3, 'b': 4, '.': 5, 'a': 6, \"'\": 7, 'f': 8, 'r': 9, 'o': 10, 'y': 11, 'i': 12, 'g': 13, 's': 14, 'u': 15, 't': 16, 'e': 17, 'k': 18, 'l': 19, ',': 20, 'w': 21, 'd': 22, ' ': 23, 'h': 24}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgQvAxXIh9rc",
        "outputId": "0ffb9049-7e33-4a5e-c15e-4056865a4bd9"
      },
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLzXKRz0iWBP"
      },
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAgwEWghjZT7",
        "outputId": "0462f115-7718-4ea4-ff9f-196928cacd3f"
      },
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAbd-Prej8A2",
        "outputId": "dc8b86ca-3397-46ec-a6b3-9e978257b74f"
      },
      "source": [
        "print(x_data[0])\n",
        "print(y_data[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[12, 8, 23, 11, 10, 15, 23, 21, 6, 3]\n",
            "[8, 23, 11, 10, 15, 23, 21, 6, 3, 16]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pii7-xWakFRK",
        "outputId": "ffcf1fb7-a252-4085-a732-eabcc6a4d2bb"
      },
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "print(x_one_hot[0])\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn5oukVrlAjg",
        "outputId": "ef30506b-b2d3-47fa-e94e-e3fe40f10dac"
      },
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNXLObhIlJXh",
        "outputId": "9e733d4d-5c74-4113-bac1-72428a342d42"
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIExOF9FlOQQ",
        "outputId": "a43911af-9dc4-4d90-9169-0d8372196f3a"
      },
      "source": [
        "print(Y[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 8, 23, 11, 10, 15, 23, 21,  6,  3, 16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDoGh0bblRQV"
      },
      "source": [
        "### 2. 모델 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQZ0P1UKlUAL"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZiAzNrOlrOA"
      },
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjWQEmf0l1gt"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcvKUpwwl3dr",
        "outputId": "c2283d83-586e-456d-dbc8-b4748583efc4"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcYqxg_l634",
        "outputId": "aa32b8e4-564b-437d-9b00-325bb1d7cfe7"
      },
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1700, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqIeJyLyoFe1",
        "outputId": "401c1d91-7dba-4097-d35b-619876035cb3"
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy86Uwioo7aa",
        "outputId": "2155eb1d-1975-43b0-bae2-cc03bfff4fa6"
      },
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\n",
            "  hh  hh hhhh hh  h  hhh h hhh   hhh   hh hh   hhhh  h  h hh hh  hh hhh   hhh  h    hhhhh  hh h hh hh hhh  h  hhhhh hh h h hh  h hh h hh hh   hh  hh h h   h hhh     hh hh hh h h  \n",
            "  w e,n  ,,,, ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, ,,e,,,, ,,,,,e,e,,,,,,s,n ,,,,,,,ee,,,,,,,,,,,,s,,,,,,,,,,,,,,,,,,,,,,,,,,,n ,,,e,,,,,,e,,,,,,,,,,s,,s,,,,e,,,,,,,,,,,,,,,,,,,,,,,,e\n",
            "t e           t       t  t             e         t     e                            t     t               to    t     t               t             t                              \n",
            "e bogoeolg bosos ototoboeogabotagbeogogt gosgogo tmg gobogtlgogob torbo tgglglbosag eogobog telbo s sottg gost  togogogo ostmslsobogostogogogtgogog gogostottgt tgosogwgohogbgobogo\n",
            "tnt st st  tmnuomt stosmt t s toe   st s ntond tlsodl ntost nt ntost so s  t mtons nt so l  h ttonts stosnstoshnt st nt smt h ststosldnh tont sm stodotm ntotont  s sout st  toeton\n",
            "mseenshest tt st te tostt toeosttttost s estnt stetsto tontontontostost shetd tonsentent n thesttst tst sheuont tontt s smt ttst tostd t ttnt stestot std sttt th   ntsttnst ttestn\n",
            "phes  t st et em sh et st shto th eh t e  tt h  sto sh eo to et to toemhshesh to sh eh ss  ehesdt  t et  het st eo  t  hesd  t t th mh t eo thesh toeo st en t sh   st tt et em st \n",
            "phtt  to t to to t  t t     t  t  t  t t  t   tt t     t  t  to t  t to  oet  t  t  t t    toet to t  t t to  o t  he  het to  o t  t to to t to  t t     t t  t        t  t tt t  \n",
            "n et  to t to to  t t t t   t     t    to te  to to    t  to to to t to   t   to    t t    tot  to t  t t to  o t   t  heo to  teto o to to   to  t t t   t   t        tt  t   t   \n",
            "n eo  ko   eo ko  t t e e   do  t eo ' eo  e  ko  o  t e  ko co  o   to   en  eo ke en     eo    o    e d to  t e   t   eo ko  teo  m eo co   to  toe eo  tn      e    eo  toe eo  \n",
            "n em  bo   wo 'o  d woe emd do  e eo ' to men bo lo  t em bo co lent won  en  eo 'e en     enem lo    end won t en  t   em bo  tem em to co   do  toeme   en   m  m    emn toeme   \n",
            "n emn lond wo lm  d wmd emd do 'e eo ' lr m n lonlo  r em lo lo lent won  en  wo 't end n  enem lo  s end won   ln  t   em loimt meem lo lonl to  toemdn  en  tm  m  s  tn t emd   \n",
            "n etn tont to lm  t tmd end to 't eoim  m tes lonto  o em lo to tent ton  end eo 't end s  toen  o ts ent ton'  lns e t em loimtst em to tonl to  t emd s end em  msis et  t emd s \n",
            "p eos tont to tos t tnt ent to 't eo '  rtte  tonto  o em to to tent won  ent to 't ent s ttoen eoi's tnt ton'  tos e t em toipt t em eo ton' to  t eod s ens am  rsis eo  t emd s \n",
            "p ea  tont to ths t tpt ept to 't to ' artteo to to  t eo to to tent won  ept wo 't ens s  toen aoi'  tnt wor   tos e t em toi t thep ao ton' ao  t epd t eps am  r is ea  thepd i \n",
            "g eo  tent wo thi t t d ept do 't do ' artteo te to  t eo to to tent won  epd wo 't d s s  toep aoi'  ant won   tos e t er toi t thep ao teng ao  t epd s ens am  r is ao  t epd s \n",
            "t eo  tond do toi d t dhepd do 't do p ardoeo tonto  t eo to to tect won  end wo 't d d s  toen toi's end won   tos e dher toi t them to tont to  t erd s ens in ersis eo  t erd s \n",
            "c eon tend wo bui d t d imt don't do m irtter te to  t eu to to tend won  end wo 't ensis  toem lo ,siend won , tos i shem to  t them lo tong wo  t eudnd ens im etsis uo  t end i \n",
            "t eon tont wo bus d and imt don't do m urtter te to  t em to bo tect won  and wo 't ans s  toem to ksiand won , tos iother toemt them to tong to  t emdnd ens im etsi  uo  t emd s \n",
            "t eos tont wo bus t andhemt don't aoum lreter le to et er to bo lect won  and won't ans e  toem toeks act won , tus i ther toemt them to bong to  themd s ens im etsis eo  themd i \n",
            "t uos tant wo 'us t t d imt dou't aoum lreteo le to et em to contect won  and don't ans g  toem toeks ant won , tus u ther toemh toem to long to  themd s ess im emsis uo  themehs \n",
            "t aor tand to bus d a dhim, don't aoum lpeteo le to et er to contect wonk and don't a s g  toem toeks and wonk, tus aather toemh toem to cong fo  thepd s e s im emsisyuo  therdhs \n",
            "tkaar tand wo bui d a dhim, don't aoum lpeteo le do et ep to co tect won  and don't a sig  toem toeks and wonk, tus lather toemh toem to cong ton themd d ess im emsityao  themdhs \n",
            "tyaas tant wo lui d a dhem, don't aoum lpeper le do et er to co lect won  and don't assig  them tosks and wonk, tus aather toemh ahem to cong fon thepdnd essiim ensit aon thepdhe \n",
            "tk oa tant wo lui d a dhim, don't aoum  ptper le do et er to co lect wor  and don't assig  toem tosks and work, tui auther toemh toem to cong fon therdnd essiimkensityuon themdet \n",
            "tk ou tant wo lui d a dhim, don't aoum  p ter le to et er to lo lect wor  and don't assig  toem tosks and work, bui uuther toemh toem to long fon themsns essiim ensityuo  themset \n",
            "tk os want wo bui d a dhim, don't aoum  pipeople do et er to lo lect word and don't assig  them tosks and work, bui uuther toemh toem to long fo  the sns essiim ensity o  themsei \n",
            "tk os want to bui d a shim, don't doum  p people to et er to lo lect word and don't dssig  them tosks and work, bui uather toemh them to long fo  the snd essiimkensity o  themses \n",
            "tk os want to bui d a shim, don't doum  p people to et er to bo lect wood and don't dssig  them tosks and wook, bui  ather toemh them to bong fon the snd essiimkens ty o  themeesm\n",
            "mk oa wont to bui d a dhic, don't drum up people toget er to bo lect wood and don't dssig  them tosks and wook, bui rather toemh them to bong fon the ent ess imkensity o  themeeim\n",
            "mktoa wont to bui d a dhic, don't drum up people together te bollect wood and don't dssig  them tosks and wook, bui rather toich them to bong fon the end essiim ensity o  the eeim\n",
            "mktoa wont to bui d a dhip, don't drum up people to ether te bollect wood and don't dssign them tosks and wook, bui rather teach them to long fon the end essiim ensity o  the seip\n",
            "mktou wont to bui d a ship, don't drum up peaple to ether to collect wood and don't dssig  them tosks and work, bui aather toach them to long fo  the end ess immensity o  the seap\n",
            "mktou want to bui d a ship, don't arum up people thgether te bollect wood and don't assig  them tosks and work, bui rather teach them to long fon the end ess immensity of the seap\n",
            "mktou want to bui d a ship, don't arum up people thgether te bollect wood and don't assign them tosks and work, bui uather teach them to bong fon the end essiimmensity of the seap\n",
            "mktou want to bui d a ship, don't arum up people together to collect word and don't assign them tosks and work, bui uather toach them to long fon the end ess immensity of the sea \n",
            "mkyou want to bui d a ship, don't arum up people together te collect word and don't assign them tosks and work, bui rather teach them to long fon the d d ess immensity of the sea \n",
            "mkyou want to build a ship, don't drum up people together to collect word and don't dssign them tosks and work, bui rather toach them to long fon the end ess immensity of the sea \n",
            "mkyou want to build a ship, don't drum up people together to lollect word and don't dssign them tosks and work, bui rather teach them to long fof the end ess immensity of the sea \n",
            "mkyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, bui rather teach them to long for the end ess immensity of the sea \n",
            "mkyou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, bui rather teach them ta long fon the end ess immensity of the sea \n",
            "mkyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, bui rather toach them to long fon the endless immensity of the sea \n",
            "pmyou want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and dork, bui rather teach the  ta long fof the endless immensity of the sea \n",
            "tkyou want wo butld a ship, don't arum up people together to bollect wood and won't assign them tosks and work, bui rather toach them to bong fon themendless immensity of themsea \n",
            "mkyou want to luild a ship, don't doum up people together to lollect wood and don't dssign ther tasks and dork, bui rather teach the  ta long fon the s dless immensity o  the sea \n",
            "mklou want to build a ship, don't drum up people to ether to bollect wood and don't dssign them tosks and work, bui rather toach them to bong fon themt t ers immensity of themd a \n",
            "tklou want to build a shic, don't arum up people tog ther to collect wood and don't assign them tosks and work, bui rather to ch them to cong fon thementlers immensity of theme a \n",
            "tkyou want to build a ship, don't arum up people together te collect word and don't assign them tosks and dork, bui rather teach them to bong fon themendless immensity of the eea \n",
            "pkyou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, bui rather teach them to long for the endless immensity of the eeap\n",
            "t you want to build a ship, don't arum up people to ether te collect wood and don't assign them tashs and work, bui rather teach them ta long for the endless immensity of the seap\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, bui rather toach them ta long for the end ess immensity of the seap\n",
            "t dou want to butld a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea \n",
            "m dou want to butld a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the seo \n",
            "m dou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, bui rather toach them to long for the endless immensity of the sea \n",
            "m dou want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and dork, bui rather toach them to long for the endless immensity of the sea \n",
            "p dou want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, bui rather teach them ta long for the endless immensity of the sea \n",
            "p you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, bui rather teach them ta long for the endless immensity of the sea \n",
            "p you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seap\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
            "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUgT8TQMtZAe"
      },
      "source": [
        "## 03. 단어 단위 RNN - 임베딩 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz1WqXastaqV"
      },
      "source": [
        "## 1. 훈련 데이터 전처리하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNubyWXVtcf6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6snNh4StfBC"
      },
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl32--Jetf0c",
        "outputId": "aa60933f-9d05-46bc-e822-9ce8538b2c94"
      },
      "source": [
        "vocab = list(set(sentence))\n",
        "print(vocab)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'medicine', 'is', 'best', 'for', 'memory', 'Repeat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gek4wzG9uYU0"
      },
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
        "word2index['<unk>']=0"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX9TKEEaufkl",
        "outputId": "ca292099-08b9-461c-937b-6c415cf1de7d"
      },
      "source": [
        "print(word2index)\n",
        "print(word2index['memory'])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'medicine': 2, 'is': 3, 'best': 4, 'for': 5, 'memory': 6, 'Repeat': 7, '<unk>': 0}\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGsg6EL2uory",
        "outputId": "5514b1c6-0094-4060-aabe-c34b4b21104b"
      },
      "source": [
        "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)\n",
        "print(index2word[6])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'the', 2: 'medicine', 3: 'is', 4: 'best', 5: 'for', 6: 'memory', 7: 'Repeat', 0: '<unk>'}\n",
            "memory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exASbXYVxCNI"
      },
      "source": [
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    return input_seq, label_seq"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US-L-9Ebywm5"
      },
      "source": [
        "X, Y = build_data(sentence, word2index)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCNdVdWZy02_",
        "outputId": "2b876af3-2b80-49b8-8476-8fedd9fcdc4c"
      },
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7, 3, 1, 4, 2, 5]])\n",
            "tensor([[3, 1, 4, 2, 5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB-NxTnnzny7"
      },
      "source": [
        "## 2. 모델 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkS4Is11zmK5"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOFlgrjRzvfE"
      },
      "source": [
        "# 하이퍼 파라미터\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OotfDQrzv__"
      },
      "source": [
        "# 모델 생성\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "# 손실함수 정의\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
        "# 옵티마이저 정의\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3Cwe-8LzxYO",
        "outputId": "86f18c29-1bcb-4307-b751-b4d2bbd53cf0"
      },
      "source": [
        "# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n",
        "output = model(X)\n",
        "print(output)\n",
        "print(output.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4200, -0.2715, -0.4658,  0.3437, -0.0088, -0.2096,  0.4254,  0.3748],\n",
            "        [-0.0051, -0.0189, -0.0288,  0.4920, -0.3981,  0.1335,  0.3078,  0.1705],\n",
            "        [ 0.4321, -0.3127, -0.1718,  0.3199, -0.1443,  0.0103,  0.3198,  0.3791],\n",
            "        [ 0.1614,  0.0650,  0.0911,  0.2352, -0.1820,  0.1905,  0.5065, -0.0500],\n",
            "        [ 0.3123, -0.4216, -0.2167,  0.3548, -0.3305, -0.2831,  0.2565,  0.3227],\n",
            "        [ 0.0901, -0.1137, -0.2355,  0.3445, -0.1750,  0.0020,  0.4150,  0.1062]],\n",
            "       grad_fn=<ViewBackward>)\n",
            "torch.Size([6, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgwLDg_Fzzbe"
      },
      "source": [
        "# 수치화된 데이터를 단어로 전환하는 함수\n",
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjHSxOnxz3it",
        "outputId": "52cf8cec-6d0b-4cfd-910c-a58dcb79c721"
      },
      "source": [
        "# 훈련 시작\n",
        "for step in range(201):\n",
        "    # 경사 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 순방향 전파\n",
        "    output = model(X)\n",
        "    # 손실값 계산\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    # 역방향 전파\n",
        "    loss.backward()\n",
        "    # 매개변수 업데이트\n",
        "    optimizer.step()\n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[01/201] 2.1214 \n",
            "Repeat memory is <unk> memory is memory\n",
            "\n",
            "[41/201] 1.5029 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[81/201] 0.8405 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.4003 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.2069 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1265 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9wweqIGz5-Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}